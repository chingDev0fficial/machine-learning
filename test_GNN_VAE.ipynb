{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a5c6cb8-c0d6-4531-8c71-883eb09c0170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from lib.lib import SignatureDataset, image_to_graph\n",
    "from typing import Tuple, Optional, Union\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.nn import GAE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "963c48d7-5860-43bd-b6ba-d57dd6b4d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, embedding_dim)\n",
    "        \n",
    "    def forward(self, x: Tensor, edge_index: Tensor, batch: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: Node features [num_nodes, in_channels]\n",
    "        edge_index: Graph edges [2, num_edges]\n",
    "        batch: Graph IDs for mini-batch training [num_nodes] - not used for GAE\n",
    "        \"\"\"\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # Return node-level embeddings for GAE\n",
    "        # Do NOT use global_mean_pool for GAE - it needs node embeddings\n",
    "        return x  # [num_nodes, embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "497cee8c-2781-488c-a88c-acdf87c83160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 signature images (genuine + forged)\n"
     ]
    }
   ],
   "source": [
    "def transform(**kwargs):\n",
    "    return transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=kwargs['num_output_channels']),\n",
    "        transforms.Resize(kwargs['resize']),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "dataset = SignatureDataset(\n",
    "    root_dir=\"test_image\",\n",
    "    transform=transform(num_output_channels=1, resize=(150, 150))\n",
    ")\n",
    "\n",
    "test_graph = []\n",
    "\n",
    "for t in dataset:\n",
    "    for test_tensor_image in t:\n",
    "        t_graph = image_to_graph(test_tensor_image)\n",
    "        test_graph.append(t_graph)\n",
    "\n",
    "next(iter(test_graph))\n",
    "next(iter(test_graph))\n",
    "\n",
    "sample = next(iter(test_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "51ed3e09-5f8c-4c65-a22f-f03ff84fba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = next(iter(test_graph)).x.shape[1]\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "epochs = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39d64602-86d0-433d-96f1-43eb9e51793c",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SignatureGNN(input_dim, hidden_dim, embedding_dim)\n",
    "model = GAE(encoder)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = model.encode(sample.x, sample.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16432930-2319-4e9b-8235-be1a68450045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1388, -0.1934,  0.1532,  ...,  0.1042,  0.1565,  0.0883],\n",
       "        [ 0.1617, -0.2071,  0.1523,  ...,  0.1083,  0.1765,  0.0973],\n",
       "        [ 0.1558, -0.2031,  0.1537,  ...,  0.1067,  0.1720,  0.0956],\n",
       "        ...,\n",
       "        [ 0.1222, -0.2086,  0.1686,  ...,  0.0904,  0.1480,  0.0965],\n",
       "        [ 0.1231, -0.2146,  0.1706,  ...,  0.0883,  0.1509,  0.0998],\n",
       "        [ 0.1050, -0.2000,  0.1693,  ...,  0.0864,  0.1346,  0.0907]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be92520f-f6d1-428a-a792-ac1dec3b615e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
