{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55ea44f6-c662-4525-9af0-8818e11ff6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, VGAE\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from lib.lib import SignatureDataset, image_to_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1975ddbb-2d4f-4f8d-9d5f-d0bb7a071e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, latent_dim):\n",
    "        super(GNNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv_mu = GCNConv(hidden_channels, latent_dim)\n",
    "        self.conv_logvar = GCNConv(hidden_channels, latent_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Step 1: Aggregate node features from neighbors\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "\n",
    "        # Step 2: Output mean and log variance\n",
    "        mu = self.conv_mu(x, edge_index)\n",
    "        logvar = self.conv_logvar(x, edge_index)\n",
    "\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c2a49f8-170a-4edd-bb14-1cdcf5a6cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SiameseNetwork(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Siamese Network for Signature Verification\n",
    "#     Takes two signature graphs and predicts if they're from the same person\n",
    "#     \"\"\"\n",
    "#     def __init__(self, gnn_vae_model, latent_dim=128, fc_hidden=256):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # Pre-trained GNN-VAE for feature extraction\n",
    "#         self.gnn_vae = gnn_vae_model\n",
    "#         self.latent_dim = latent_dim\n",
    "        \n",
    "#         # Freeze GNN-VAE weights (optional - for fine-tuning, set to False)\n",
    "#         for param in self.gnn_vae.parameters():\n",
    "#             param.requires_grad = False\n",
    "        \n",
    "#         # Similarity computation layers\n",
    "#         self.fc1 = nn.Linear(latent_dim * 2, fc_hidden)\n",
    "#         self.bn1 = nn.BatchNorm1d(fc_hidden)\n",
    "        \n",
    "#         self.fc2 = nn.Linear(fc_hidden, fc_hidden // 2)\n",
    "#         self.bn2 = nn.BatchNorm1d(fc_hidden // 2)\n",
    "        \n",
    "#         self.fc3 = nn.Linear(fc_hidden // 2, 1)\n",
    "        \n",
    "#         self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "#     def forward_one(self, x, edge_index, batch=None):\n",
    "#         \"\"\"\n",
    "#         Extract features from one signature graph\n",
    "#         \"\"\"\n",
    "#         # Use GNN-VAE's extract_features method\n",
    "#         with torch.no_grad():  # Don't update GNN-VAE weights\n",
    "#             embedding = self.gnn_vae.encode(x, edge_index)\n",
    "        \n",
    "#         return embedding\n",
    "    \n",
    "#     def forward(self, x1, edge_index1, x2, edge_index2, batch1=None, batch2=None):\n",
    "#         \"\"\"\n",
    "#         Forward pass for two signatures\n",
    "        \n",
    "#         Args:\n",
    "#             x1, edge_index1: First signature graph\n",
    "#             x2, edge_index2: Second signature graph\n",
    "#             batch1, batch2: Batch indices (if processing multiple pairs)\n",
    "        \n",
    "#         Returns:\n",
    "#             similarity: Similarity score [0, 1]\n",
    "#             emb1, emb2: Embeddings for analysis\n",
    "#         \"\"\"\n",
    "#         # Extract embeddings from both signatures\n",
    "#         emb1 = self.forward_one(x1, edge_index1, batch1)  # [batch_size, latent_dim]\n",
    "#         emb2 = self.forward_one(x2, edge_index2, batch2)  # [batch_size, latent_dim]\n",
    "        \n",
    "#         # Concatenate embeddings\n",
    "#         combined = torch.cat([emb1, emb2], dim=1)  # [batch_size, latent_dim * 2]\n",
    "        \n",
    "#         # Pass through similarity network\n",
    "#         x = self.fc1(combined)\n",
    "#         if x.size(0) > 1:\n",
    "#             x = self.bn1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "#         x = self.fc2(x)\n",
    "#         if x.size(0) > 1:\n",
    "#             x = self.bn2(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "#         # Output similarity score\n",
    "#         similarity = torch.sigmoid(self.fc3(x))  # [batch_size, 1]\n",
    "        \n",
    "#         return similarity, emb1, emb2\n",
    "    \n",
    "#     def predict(self, x1, edge_index1, x2, edge_index2):\n",
    "#         \"\"\"\n",
    "#         Predict if two signatures are from the same person\n",
    "        \n",
    "#         Returns:\n",
    "#             is_same_person: Boolean prediction\n",
    "#             similarity_score: Confidence score\n",
    "#         \"\"\"\n",
    "#         self.eval()\n",
    "#         with torch.no_grad():\n",
    "#             similarity, _, _ = self.forward(x1, edge_index1, x2, edge_index2)\n",
    "#             similarity = similarity.mean()\n",
    "#             is_same_person = similarity > 0.5\n",
    "#             return is_same_person.item(), similarity.item()\n",
    "    \n",
    "#     def get_embedding(self, x, edge_index, batch=None):\n",
    "#         \"\"\"\n",
    "#         Get embedding for a single signature\n",
    "#         \"\"\"\n",
    "#         self.eval()\n",
    "#         with torch.no_grad():\n",
    "#             return self.forward_one(x, edge_index, batch)\n",
    "\n",
    "class ContrastiveSiameseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Siamese Network using Contrastive Loss\n",
    "    Better for signature verification with distance-based similarity\n",
    "    \"\"\"\n",
    "    def __init__(self, gnn_vae_model, latent_dim=32, margin=2.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gnn_vae = gnn_vae_model\n",
    "        self.latent_dim = latent_dim\n",
    "        self.margin = margin  # Margin for contrastive loss\n",
    "        \n",
    "        # Freeze GNN-VAE\n",
    "        for param in self.gnn_vae.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Optional: Additional projection layer\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "    \n",
    "    def forward_one(self, x, edge_index, batch=None):\n",
    "        \"\"\"Extract and project features\"\"\"\n",
    "        with torch.no_grad():\n",
    "            embedding = self.gnn_vae.encode(x, edge_index)\n",
    "        \n",
    "        # Optional projection\n",
    "        embedding = self.projection(embedding)\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def forward(self, x1, edge_index1, x2, edge_index2, batch1=None, batch2=None):\n",
    "        \"\"\"\n",
    "        Returns embeddings and Euclidean distance\n",
    "        \"\"\"\n",
    "        emb1 = self.forward_one(x1, edge_index1, batch1)\n",
    "        emb2 = self.forward_one(x2, edge_index2, batch2)\n",
    "        \n",
    "        # Compute Euclidean distance\n",
    "        distance = F.pairwise_distance(emb1, emb2)\n",
    "        \n",
    "        return distance, emb1, emb2\n",
    "    \n",
    "    def predict(self, x1, edge_index1, x2, edge_index2, threshold=1.0):\n",
    "        \"\"\"\n",
    "        Predict based on distance threshold\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            distance, _, _ = self.forward(x1, edge_index1, x2, edge_index2)\n",
    "            distance = distance.mean()\n",
    "            is_same_person = distance < threshold\n",
    "            return is_same_person.item(), distance.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5696fd37-5657-4a57-bd02-f1ef0be41d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 signature images (genuine + forged)\n"
     ]
    }
   ],
   "source": [
    "def transform(**kwargs):\n",
    "    return transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=kwargs['num_output_channels']),\n",
    "        transforms.Resize(kwargs['resize']),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "dataset = SignatureDataset(\n",
    "    root_dir=\"test_image\",\n",
    "    transform=transform(num_output_channels=1, resize=(150, 150))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0d5e2b11-a45f-4ec7-b433-2893487549bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train graphs: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "test_graph = []\n",
    "\n",
    "# Convert training dataset\n",
    "for t in tqdm(dataset, desc=\"Train Graphs\", leave=False):\n",
    "    t_graph = image_to_graph(t)\n",
    "    test_graph.append(t_graph)\n",
    "\n",
    "print(\"Train graphs:\", len(test_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0961ab42-2f78-40b8-802b-690d720eed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = next(iter(test_graph)).x.shape[1]\n",
    "hidden_dim = 64\n",
    "latent_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fbe4fad6-e0a6-4459-89d6-d6f97c582ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('VGAE_Model.pt')\n",
    "gnn_vae = VGAE(GNNEncoder(in_channels=input_dim, hidden_channels=hidden_dim, latent_dim=latent_dim))\n",
    "gnn_vae.load_state_dict(checkpoint)\n",
    "gnn_vae.eval()\n",
    "\n",
    "# Create Siamese Network\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# siamese_model = SiameseNetwork(\n",
    "#     gnn_vae_model=gnn_vae,\n",
    "#     latent_dim=128,\n",
    "#     fc_hidden=256\n",
    "# ).to(device)\n",
    "\n",
    "contrastive_model = ContrastiveSiameseNetwork(\n",
    "    gnn_vae_model=gnn_vae,\n",
    "    latent_dim=128,\n",
    "    margin=2.0\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c0a269d1-d7bb-4318-89cf-d04b136d6def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with two signatures\n",
    "graph1 = test_graph[1].to(device)\n",
    "graph2 = test_graph[0].to(device)\n",
    "\n",
    "# Predict similarity\n",
    "# is_same, score = siamese_model.predict(\n",
    "#     graph1.x, graph1.edge_index,\n",
    "#     graph2.x, graph2.edge_index\n",
    "# )\n",
    "\n",
    "is_same, score = contrastive_model.predict(\n",
    "    graph1.x, graph1.edge_index,\n",
    "    graph2.x, graph2.edge_index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d84f86b8-eff5-49a7-84d1-8ddb73bdd03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8d6cc744-7c78-4b77-8617-591c38ef69fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09018753468990326"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d55430-a326-4bc4-9ee8-a7883d41e057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b697946-ef0b-457e-9a80-7fedecc60772",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
