{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03065efc-104b-4d65-83b8-854a096e2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import image\n",
    "import cv2\n",
    "from typing import Tuple, Optional, Union\n",
    "\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0462bb-49ac-4ee4-8a2a-8bfe3287644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32  # Changed to match your DataLoader batch_size\n",
    "epochs = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e608522-89e0-4d3f-a518-ff71a72c5b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, embedding_dim)\n",
    "        \n",
    "    def forward(self, x: Tensor, edge_index: Tensor, batch: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: Node features [num_nodes, in_channels]\n",
    "        edge_index: Graph edges [2, num_edges]\n",
    "        batch: Graph IDs for mini-batch training [num_nodes]\n",
    "        \"\"\"\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # Aggregate node embeddings into a graph-level signature embedding\n",
    "        x = global_mean_pool(x, batch)  # [num_graphs, embedding_dim]\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdfc94a5-3882-4e76-bc57-0fb11792728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        # collect all signer folders\n",
    "        signer_folders = sorted(os.listdir(root_dir))\n",
    "\n",
    "        for folder in signer_folders:\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "            if os.path.isdir(folder_path):\n",
    "                for img_name in os.listdir(folder_path):\n",
    "                    if self._is_image_file(img_name):\n",
    "                        self.samples.append(os.path.join(folder_path, img_name))\n",
    "\n",
    "        print(f\"Loaded {len(self.samples)} signature images (genuine + forged)\")\n",
    "\n",
    "    def _is_image_file(self, filename):\n",
    "        valid_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "        return os.path.splitext(filename.lower())[1] in valid_exts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(path).convert(\"L\")  # grayscale\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image   # only image, no label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            # fallback blank image\n",
    "            fallback = Image.new(\"L\", (224, 224), 0)\n",
    "            if self.transform:\n",
    "                fallback = self.transform(fallback)\n",
    "            return fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6337dc5a-b42a-476c-904f-4200e6fccd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_graph(\n",
    "    image_tensor: torch.Tensor,\n",
    "    patch_size: int = 8,\n",
    "    k_neighbors: int = 8,\n",
    "    edge_threshold: float = 0.1,\n",
    "    include_features: bool = True\n",
    ") -> Data:\n",
    "    \"\"\"\n",
    "    Convert an image to a graph representation with nodes and edges.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor: Input image tensor of shape (C, H, W) or (H, W)\n",
    "        method: Graph construction method ('grid', 'knn', 'superpixel', 'region')\n",
    "        patch_size: Size of patches for grid method\n",
    "        k_neighbors: Number of neighbors for KNN method\n",
    "        edge_threshold: Threshold for edge creation based on feature similarity\n",
    "        include_features: Whether to include patch features as node features\n",
    "        \n",
    "    Returns:\n",
    "        PyTorch Geometric Data object with node features and edge indices\n",
    "    \"\"\"\n",
    "    \n",
    "    return _image_to_grid_graph(image_tensor, patch_size, include_features)\n",
    "\n",
    "def _image_to_grid_graph(\n",
    "    image_tensor: torch.Tensor, \n",
    "    patch_size: int, \n",
    "    include_features: bool\n",
    ") -> Data:\n",
    "    \"\"\"Convert image to grid-based graph where each patch is a node.\"\"\"\n",
    "    \n",
    "    # Handle different input shapes\n",
    "    if len(image_tensor.shape) == 2:\n",
    "        image_tensor = image_tensor.unsqueeze(0)  # Add channel dimension\n",
    "    \n",
    "    C, H, W = image_tensor.shape\n",
    "    \n",
    "    # Create patches\n",
    "    patches_h = H // patch_size\n",
    "    patches_w = W // patch_size\n",
    "    \n",
    "    # Extract patch features\n",
    "    node_features = []\n",
    "    node_positions = []\n",
    "    \n",
    "    for i in range(patches_h):\n",
    "        for j in range(patches_w):\n",
    "            # Extract patch\n",
    "            patch = image_tensor[\n",
    "                :, \n",
    "                i * patch_size:(i + 1) * patch_size,\n",
    "                j * patch_size:(j + 1) * patch_size\n",
    "            ]\n",
    "            \n",
    "            if include_features:\n",
    "                # Compute patch statistics as features\n",
    "                mean_val = patch.mean(dim=[1, 2])  # Per channel mean\n",
    "                std_val = patch.std(dim=[1, 2])    # Per channel std\n",
    "                max_val = patch.max(dim=2)[0].max(dim=1)[0]  # Per channel max\n",
    "                min_val = patch.min(dim=2)[0].min(dim=1)[0]  # Per channel min\n",
    "                \n",
    "                features = torch.cat([mean_val, std_val, max_val, min_val])\n",
    "                node_features.append(features)\n",
    "            \n",
    "            # Store position\n",
    "            node_positions.append([i, j])\n",
    "    \n",
    "    # Create edges (connect adjacent patches)\n",
    "    edge_indices = []\n",
    "    \n",
    "    for i in range(patches_h):\n",
    "        for j in range(patches_w):\n",
    "            current_node = i * patches_w + j\n",
    "            \n",
    "            # Connect to neighbors (4-connectivity)\n",
    "            neighbors = [\n",
    "                (i-1, j), (i+1, j),  # vertical neighbors\n",
    "                (i, j-1), (i, j+1)   # horizontal neighbors\n",
    "            ]\n",
    "            \n",
    "            # Add diagonal connections for 8-connectivity\n",
    "            neighbors.extend([\n",
    "                (i-1, j-1), (i-1, j+1),\n",
    "                (i+1, j-1), (i+1, j+1)\n",
    "            ])\n",
    "            \n",
    "            for ni, nj in neighbors:\n",
    "                if 0 <= ni < patches_h and 0 <= nj < patches_w:\n",
    "                    neighbor_node = ni * patches_w + nj\n",
    "                    edge_indices.append([current_node, neighbor_node])\n",
    "    \n",
    "    # Convert to tensors\n",
    "    if include_features:\n",
    "        x = torch.stack(node_features)\n",
    "    else:\n",
    "        x = torch.tensor(node_positions, dtype=torch.float32)\n",
    "    \n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "    pos = torch.tensor(node_positions, dtype=torch.float32)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a52932-36d1-484a-a98a-c322ea3b5f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14626 signature images (genuine + forged)\n"
     ]
    }
   ],
   "source": [
    "def dataset_path():\n",
    "    path = kagglehub.dataset_download(\"akashgundu/signature-verification-dataset\")\n",
    "    return os.path.join(path, 'extract')\n",
    "\n",
    "def transform(**kwargs):\n",
    "    return transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=kwargs['num_output_channels']),\n",
    "        transforms.Resize(kwargs['resize']),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "dataset = SignatureDataset(\n",
    "    root_dir=dataset_path(),\n",
    "    transform=transform(num_output_channels=1, resize=(150, 150))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1943df6-ffa8-48fe-9e99-3dd144eb87df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes - Train: 11700, Validation: 2926\n"
     ]
    }
   ],
   "source": [
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "print(f\"Dataset sizes - Train: {train_size}, Validation: {val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e87200dd-9edf-4d18-ad1a-e72e8b644073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2926it [03:05, 15.78it/s]\n"
     ]
    }
   ],
   "source": [
    "train_graph = []\n",
    "val_graph = []\n",
    "\n",
    "for t, v in tqdm(zip(train_dataset, val_dataset)):\n",
    "    for train_tensor_image, val_tensor_image in zip(t, v):\n",
    "        t_graph = image_to_graph(train_tensor_image)\n",
    "        v_graph = image_to_graph(val_tensor_image)\n",
    "        train_graph.append(t_graph)\n",
    "        val_graph.append(v_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6eb5c5b-2e86-4f82-b909-f1d213071b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph = DataLoader(train_graph, batch_size=batch_size, shuffle=True)\n",
    "val_graph = DataLoader(val_graph, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c6fa7ca-8100-496c-849c-42c2864213d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(graphs, model, epochs=50, optimizer):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "def test_loop(graphs, model, loss_type=\"reconstruction\", **kwargs):\n",
    "    \"\"\"Simplified testing loop for unsupervised GNNs\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for graph in tqdm(graphs, desc=\"Testing\"):\n",
    "            embedding = model(graph.x, graph.edge_index, getattr(graph, \"batch\", None))\n",
    "\n",
    "            if loss_type == \"reconstruction\":\n",
    "                loss = reconstruction_loss(embedding, graph.edge_index)\n",
    "            elif loss_type == \"contrastive\":\n",
    "                loss = contrastive_loss(embedding, temperature=kwargs.get(\"temperature\", 0.1))\n",
    "            elif loss_type == \"edge_prediction\":\n",
    "                loss = edge_prediction_loss(embedding, graph.edge_index)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown loss type: {loss_type}\")\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(graphs)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "# -------- Losses -------- #\n",
    "\n",
    "def reconstruction_loss(embeddings, edge_index):\n",
    "    num_nodes = embeddings.size(0)\n",
    "    sim_matrix = torch.sigmoid(embeddings @ embeddings.t())\n",
    "\n",
    "    adj = torch.zeros(num_nodes, num_nodes, device=embeddings.device)\n",
    "\n",
    "    # Filter edges so they don’t exceed num_nodes\n",
    "    mask = (edge_index[0] < num_nodes) & (edge_index[1] < num_nodes)\n",
    "    edge_index = edge_index[:, mask]\n",
    "\n",
    "    adj[edge_index[0], edge_index[1]] = 1\n",
    "\n",
    "    return F.binary_cross_entropy(sim_matrix, adj)\n",
    "\n",
    "\n",
    "def contrastive_loss(embeddings, temperature=0.1):\n",
    "    \"\"\"Self-supervised contrastive loss\"\"\"\n",
    "    z = F.normalize(embeddings, dim=1)\n",
    "    sim = z @ z.t() / temperature\n",
    "    labels = torch.arange(z.size(0), device=z.device)\n",
    "    return F.cross_entropy(sim, labels)\n",
    "\n",
    "\n",
    "def edge_prediction_loss(embeddings, edge_index):\n",
    "    \"\"\"Binary link prediction loss\"\"\"\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index, num_nodes=embeddings.size(0),\n",
    "        num_neg_samples=edge_index.size(1)\n",
    "    )\n",
    "\n",
    "    pos_scores = (embeddings[edge_index[0]] * embeddings[edge_index[1]]).sum(dim=1)\n",
    "    neg_scores = (embeddings[neg_edge_index[0]] * embeddings[neg_edge_index[1]]).sum(dim=1)\n",
    "\n",
    "    pos_loss = F.binary_cross_entropy_with_logits(pos_scores, torch.ones_like(pos_scores))\n",
    "    neg_loss = F.binary_cross_entropy_with_logits(neg_scores, torch.zeros_like(neg_scores))\n",
    "    return (pos_loss + neg_loss) / 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6156fd0d-e24b-4070-bc1d-b5b02532d22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/signature_gnn')\n",
    "\n",
    "input_dim = next(iter(train_graph)).x.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = 128\n",
    "\n",
    "model = SignatureGCN(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d6f2596-ad31-4abd-a78d-5abd12acf257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[10368, 4], edge_index=[2, 76160], pos=[10368, 2], batch=[10368], ptr=[33])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f27ffd2-0e9e-4648-a104-d97d16b50199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize best loss for unsupervised learning (lower is better)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training - corrected function call for unsupervised learning\n",
    "    train_loss = train_loop(train_graph, model, optimizer, loss_type='reconstruction')\n",
    "    \n",
    "    # Validation - corrected function call for unsupervised learning  \n",
    "    val_loss = test_loop(val_graph, model, loss_type='reconstruction')\n",
    "    \n",
    "    # Logging - only loss since we don't have accuracy in unsupervised learning\n",
    "    writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Save best model based on validation loss (lower is better)\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_feature_extraction_model.pth')\n",
    "        print(f\"✓ New best model saved with validation loss: {val_loss:.4f}\")\n",
    "\n",
    "writer.close()\n",
    "print(f\"\\nTraining completed! Best validation loss: {best_val_loss:.4f}\")\n",
    "print(\"Model saved as 'best_feature_extraction_model.pth'\")\n",
    "\n",
    "# Alternative: If reconstruction still gives issues, use edge_prediction\n",
    "# Just change 'reconstruction' to 'edge_prediction' in both function calls:\n",
    "# train_loss = train_loop(train_graph, model, optimizer, loss_type='edge_prediction')\n",
    "# val_loss = test_loop(val_graph, model, loss_type='edge_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c75a9-a718-4931-a566-0fe071c1d261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f3573-6944-4c4c-921e-3a3444fd10d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
