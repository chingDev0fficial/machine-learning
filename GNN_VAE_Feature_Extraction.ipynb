{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c738a5d5-0e54-411a-ae92-9658502e9907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from typing import Tuple, Optional, Union\n",
    "from sklearn.feature_extraction import image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.nn import GCNConv, VGAE, global_mean_pool, global_max_pool, global_add_pool\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib import tmap\n",
    "\n",
    "from lib.lib import SignatureDataset, image_to_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30c3bad8-7a53-4957-b1fc-5087abc26140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, latent_dim):\n",
    "        super(GNNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv_mu = GCNConv(hidden_channels, latent_dim)\n",
    "        self.conv_logvar = GCNConv(hidden_channels, latent_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Step 1: Aggregate node features from neighbors\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "\n",
    "        # Step 2: Output mean and log variance\n",
    "        mu = self.conv_mu(x, edge_index)\n",
    "        logvar = self.conv_logvar(x, edge_index)\n",
    "\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc6ac3f2-48a2-4090-918b-b73a0337447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "w_d = 1e-5\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55baef96-f4dd-426f-87d8-91b3f465415d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14626 signature images (genuine + forged)\n"
     ]
    }
   ],
   "source": [
    "def dataset_path():\n",
    "    path = kagglehub.dataset_download(\"akashgundu/signature-verification-dataset\")\n",
    "    return os.path.join(path, 'extract')\n",
    "\n",
    "def transform(**kwargs):\n",
    "    return transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=kwargs['num_output_channels']),\n",
    "        transforms.Resize(kwargs['resize']),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "dataset = SignatureDataset(\n",
    "    root_dir=dataset_path(),\n",
    "    transform=transform(num_output_channels=1, resize=(150, 150))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b627ec6d-073d-4e21-a82f-49043e6fc5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9373, 0.9333, 0.9961,  ..., 0.9490, 0.9529, 0.9804],\n",
       "         [0.9490, 0.9686, 0.9882,  ..., 0.9961, 0.9922, 0.9922],\n",
       "         [0.9765, 0.9765, 0.9137,  ..., 0.9922, 1.0000, 0.9961],\n",
       "         ...,\n",
       "         [0.9569, 0.9961, 0.9961,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.9529, 0.9804, 0.9882,  ..., 1.0000, 1.0000, 1.0000],\n",
       "         [0.9490, 0.9412, 0.9765,  ..., 0.9922, 0.9922, 0.9961]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8d484f5-f6ad-4254-b8d6-2d490db56dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes - Train: 11700, Validation: 2926\n"
     ]
    }
   ],
   "source": [
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "print(f\"Dataset sizes - Train: {train_size}, Validation: {val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79e51521-93cb-40cf-901f-5955e569e622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e02ce3-39e5-43c3-98bb-9d081ab6eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## def graph_converter(data):\n",
    "    d_graph = image_to_graph(data)\n",
    "    return d_graph   \n",
    "\n",
    "# Convert training dataset\n",
    "train_graph = list(tmap(graph_converter, train_dataset, desc=\"Train Graphs\"))\n",
    "\n",
    "# Convert validation dataset\n",
    "val_graph = list(tmap(graph_converter, val_dataset, desc=\"Val Graphs\"))\n",
    "\n",
    "print(\"Train graphs:\", len(train_graph))\n",
    "print(\"Val graphs:\", len(val_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2d16694-9f69-4d88-935c-0e90b0adac37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[32768, 3], edge_index=[2, 126976], batch=[32768], ptr=[33])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_graph,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=graph_collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_graph,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=graph_collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5911923d-6e34-4a09-97bb-e49eeb7841c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from datetime import datetime\n",
    "\n",
    "# Create unique writer for each run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "writer = SummaryWriter(f'runs/gae_experiment_{timestamp}')\n",
    "\n",
    "def train_step(model, optimizer, data_list, beta=1.0):\n",
    "    \"\"\"\n",
    "    Training step for a list of graph Data objects.\n",
    "    Each graph is trained independently.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for data in tqdm(data_list, desc=\"Training\", leave=False):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Encode nodes into latent space\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "\n",
    "        # Reconstruction + KL loss\n",
    "        recon_loss = model.recon_loss(z, data.edge_index)\n",
    "        kl_loss = (1 / data.num_nodes) * model.kl_loss()\n",
    "        loss = recon_loss + beta * kl_loss\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(data_list)\n",
    "    return avg_loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_step(model, val_list, beta=1.0):\n",
    "    \"\"\"\n",
    "    Validation step for a list of graph Data objects.\n",
    "    No gradient updates are performed.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for data in tqdm(val_list, desc=\"Validating\", leave=False):\n",
    "        z = model.encode(data.x, data.edge_index)\n",
    "\n",
    "        recon_loss = model.recon_loss(z, data.edge_index)\n",
    "        kl_loss = (1 / data.num_nodes) * model.kl_loss()\n",
    "        loss = recon_loss + beta * kl_loss\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(val_list)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88a5bac3-e843-40b6-b8e1-9f803ddb1a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = next(iter(train_graph)).x.shape[1]\n",
    "hidden_dim = 64\n",
    "latent_dim = 128\n",
    "# epochs = 500\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cf7a501-9401-40db-bc8d-85b988349adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = VGAE(GNNEncoder(in_channels=input_dim, hidden_channels=hidden_dim, latent_dim=latent_dim))\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77eb9324-a16f-4922-9478-2b00f89f181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_vgae(model, optimizer, train_loader, val_loader, epochs=100, beta=1.0, log_dir=\"runs/vgae_signature\"):\n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 10  # early stopping patience\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_step(model, optimizer, train_loader, beta)\n",
    "        val_loss = val_step(model, val_loader, beta)\n",
    "\n",
    "        # Log to TensorBoard\n",
    "        writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "        writer.add_scalar(\"KL_Beta\", beta, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch:03d} | Train: {train_loss:.4f} | Val: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping based on validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            wait = 0\n",
    "            torch.save(model.state_dict(), os.path.join(log_dir, \"best_vgae_model.pth\"))\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"⏹️ Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "    print(\"✅ Training finished. Best model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "673ffbd9-a18e-4109-9c3a-6fed89cb2d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train: 2.0481 | Val: 1.0858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 | Train: 1.0412 | Val: 0.9977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 | Train: 0.9863 | Val: 0.9702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 | Train: 0.9685 | Val: 0.9634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 | Train: 0.9638 | Val: 0.9595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "vgae = train_vgae(model, optimizer, train_loader, val_loader, epochs=epochs, beta=1.0, log_dir=\"runs/vgae_signature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e88e7251-8f7e-471f-9f13-644b8fa4dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'VGAE_Model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25df48c2-20a2-4f7c-a941-1fff7841b2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
