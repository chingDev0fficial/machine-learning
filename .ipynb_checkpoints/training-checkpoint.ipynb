{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03065efc-104b-4d65-83b8-854a096e2c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import image\n",
    "import cv2\n",
    "from typing import Tuple, Optional, Union\n",
    "\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d0462bb-49ac-4ee4-8a2a-8bfe3287644c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32  # Changed to match your DataLoader batch_size\n",
    "epochs = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e608522-89e0-4d3f-a518-ff71a72c5b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, embedding_dim)\n",
    "        \n",
    "    def forward(self, x: Tensor, edge_index: Tensor, batch: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: Node features [num_nodes, in_channels]\n",
    "        edge_index: Graph edges [2, num_edges]\n",
    "        batch: Graph IDs for mini-batch training [num_nodes]\n",
    "        \"\"\"\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # Aggregate node embeddings into a graph-level signature embedding\n",
    "        x = global_mean_pool(x, batch)  # [num_graphs, embedding_dim]\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bdfc94a5-3882-4e76-bc57-0fb11792728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        # collect all signer folders\n",
    "        signer_folders = sorted(os.listdir(root_dir))\n",
    "\n",
    "        for folder in signer_folders:\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "            if os.path.isdir(folder_path):\n",
    "                for img_name in os.listdir(folder_path):\n",
    "                    if self._is_image_file(img_name):\n",
    "                        self.samples.append(os.path.join(folder_path, img_name))\n",
    "\n",
    "        print(f\"Loaded {len(self.samples)} signature images (genuine + forged)\")\n",
    "\n",
    "    def _is_image_file(self, filename):\n",
    "        valid_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "        return os.path.splitext(filename.lower())[1] in valid_exts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(path).convert(\"L\")  # grayscale\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image   # only image, no label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            # fallback blank image\n",
    "            fallback = Image.new(\"L\", (224, 224), 0)\n",
    "            if self.transform:\n",
    "                fallback = self.transform(fallback)\n",
    "            return fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6337dc5a-b42a-476c-904f-4200e6fccd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_graph(\n",
    "    image_tensor: torch.Tensor,\n",
    "    patch_size: int = 8,\n",
    "    k_neighbors: int = 8,\n",
    "    edge_threshold: float = 0.1,\n",
    "    include_features: bool = True\n",
    ") -> Data:\n",
    "    \"\"\"\n",
    "    Convert an image to a graph representation with nodes and edges.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor: Input image tensor of shape (C, H, W) or (H, W)\n",
    "        method: Graph construction method ('grid', 'knn', 'superpixel', 'region')\n",
    "        patch_size: Size of patches for grid method\n",
    "        k_neighbors: Number of neighbors for KNN method\n",
    "        edge_threshold: Threshold for edge creation based on feature similarity\n",
    "        include_features: Whether to include patch features as node features\n",
    "        \n",
    "    Returns:\n",
    "        PyTorch Geometric Data object with node features and edge indices\n",
    "    \"\"\"\n",
    "    \n",
    "    return _image_to_grid_graph(image_tensor, patch_size, include_features)\n",
    "\n",
    "def _image_to_grid_graph(\n",
    "    image_tensor: torch.Tensor, \n",
    "    patch_size: int, \n",
    "    include_features: bool\n",
    ") -> Data:\n",
    "    \"\"\"Convert image to grid-based graph where each patch is a node.\"\"\"\n",
    "    \n",
    "    # Handle different input shapes\n",
    "    if len(image_tensor.shape) == 2:\n",
    "        image_tensor = image_tensor.unsqueeze(0)  # Add channel dimension\n",
    "    \n",
    "    C, H, W = image_tensor.shape\n",
    "    \n",
    "    # Create patches\n",
    "    patches_h = H // patch_size\n",
    "    patches_w = W // patch_size\n",
    "    \n",
    "    # Extract patch features\n",
    "    node_features = []\n",
    "    node_positions = []\n",
    "    \n",
    "    for i in range(patches_h):\n",
    "        for j in range(patches_w):\n",
    "            # Extract patch\n",
    "            patch = image_tensor[\n",
    "                :, \n",
    "                i * patch_size:(i + 1) * patch_size,\n",
    "                j * patch_size:(j + 1) * patch_size\n",
    "            ]\n",
    "            \n",
    "            if include_features:\n",
    "                # Compute patch statistics as features\n",
    "                mean_val = patch.mean(dim=[1, 2])  # Per channel mean\n",
    "                std_val = patch.std(dim=[1, 2])    # Per channel std\n",
    "                max_val = patch.max(dim=2)[0].max(dim=1)[0]  # Per channel max\n",
    "                min_val = patch.min(dim=2)[0].min(dim=1)[0]  # Per channel min\n",
    "                \n",
    "                features = torch.cat([mean_val, std_val, max_val, min_val])\n",
    "                node_features.append(features)\n",
    "            \n",
    "            # Store position\n",
    "            node_positions.append([i, j])\n",
    "    \n",
    "    # Create edges (connect adjacent patches)\n",
    "    edge_indices = []\n",
    "    \n",
    "    for i in range(patches_h):\n",
    "        for j in range(patches_w):\n",
    "            current_node = i * patches_w + j\n",
    "            \n",
    "            # Connect to neighbors (4-connectivity)\n",
    "            neighbors = [\n",
    "                (i-1, j), (i+1, j),  # vertical neighbors\n",
    "                (i, j-1), (i, j+1)   # horizontal neighbors\n",
    "            ]\n",
    "            \n",
    "            # Add diagonal connections for 8-connectivity\n",
    "            neighbors.extend([\n",
    "                (i-1, j-1), (i-1, j+1),\n",
    "                (i+1, j-1), (i+1, j+1)\n",
    "            ])\n",
    "            \n",
    "            for ni, nj in neighbors:\n",
    "                if 0 <= ni < patches_h and 0 <= nj < patches_w:\n",
    "                    neighbor_node = ni * patches_w + nj\n",
    "                    edge_indices.append([current_node, neighbor_node])\n",
    "    \n",
    "    # Convert to tensors\n",
    "    if include_features:\n",
    "        x = torch.stack(node_features)\n",
    "    else:\n",
    "        x = torch.tensor(node_positions, dtype=torch.float32)\n",
    "    \n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "    pos = torch.tensor(node_positions, dtype=torch.float32)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "54a52932-36d1-484a-a98a-c322ea3b5f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14626 signature images (genuine + forged)\n"
     ]
    }
   ],
   "source": [
    "def dataset_path():\n",
    "    path = kagglehub.dataset_download(\"akashgundu/signature-verification-dataset\")\n",
    "    return os.path.join(path, 'extract')\n",
    "\n",
    "def transform(**kwargs):\n",
    "    return transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=kwargs['num_output_channels']),\n",
    "        transforms.Resize(kwargs['resize']),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "dataset = SignatureDataset(\n",
    "    root_dir=dataset_path(),\n",
    "    transform=transform(num_output_channels=1, resize=(150, 150))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f1943df6-ffa8-48fe-9e99-3dd144eb87df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes - Train: 11700, Validation: 2926\n"
     ]
    }
   ],
   "source": [
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "print(f\"Dataset sizes - Train: {train_size}, Validation: {val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e87200dd-9edf-4d18-ad1a-e72e8b644073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [00:07, 11.89it/s]\n"
     ]
    }
   ],
   "source": [
    "train_graph = []\n",
    "val_graph = []\n",
    "\n",
    "for t, v in tqdm(zip(train_dataset, val_loader)):\n",
    "    for train_tensor_image, val_tensor_image in zip(t, v):\n",
    "        t_graph = image_to_graph(train_tensor_image)\n",
    "        v_graph = image_to_graph(val_tensor_image)\n",
    "        train_graph.append(t_graph)\n",
    "        val_graph.append(v_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0c6fa7ca-8100-496c-849c-42c2864213d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(graphs, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for graph_idx, graph in enumerate(tqdm(graphs, desc=\"Training\")):\n",
    "        batch = next(iter(t_batch))\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        graph_loss = 0\n",
    "        graph_correct = 0\n",
    "\n",
    "        embedding = model(graph.x, graph.edge_index, graph.pos)\n",
    "\n",
    "        # Calculate loss\n",
    "        if embedding.dim() == 1:\n",
    "            embedding = embedding.unsqueeze(0)\n",
    "        loss = loss_fn(embedding, label)\n",
    "        graph_loss += loss\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        if embedding.size(-1) > 1:  # Multi-class\n",
    "            pred_class = embedding.argmax(dim=-1)\n",
    "            graph_correct += (pred_class == label).sum().item()\n",
    "        \n",
    "        if graph_loss > 0:\n",
    "            graph_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += graph_loss.item()\n",
    "            correct += graph_correct\n",
    "            total_samples += len(graphs)\n",
    "        \n",
    "        if graph_idx % 10 == 0:\n",
    "            avg_loss = total_loss / (graph_idx + 1) if graph_idx > 0 else graph_loss.item()\n",
    "            print(f\"Graph {graph_idx}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(graphs) if len(graphs) > 0 else 0\n",
    "    accuracy = correct / total_samples if total_samples > 0 else 0\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def test_loop(graphs, model, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for graph in tqdm(graphs, desc=\"Testing\"):\n",
    "            batch = next(iter(v_batch))\n",
    "            \n",
    "            graph_loss = 0\n",
    "            graph_correct = 0\n",
    "                    \n",
    "            # Get prediction\n",
    "            embedding = model(graph.x, graph.edge_index, graph.pos)\n",
    "            \n",
    "            # Calculate loss\n",
    "            if embedding.dim() == 1:\n",
    "                embedding = embedding.unsqueeze(0)\n",
    "            loss = loss_fn(embedding, label)\n",
    "            graph_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            if embedding.size(-1) > 1:\n",
    "                pred_class = embedding.argmax(dim=-1)\n",
    "                graph_correct += (pred_class == label).sum().item()\n",
    "            \n",
    "            total_loss += graph_loss\n",
    "            correct += graph_correct\n",
    "            total_samples += len(graphs)\n",
    "    \n",
    "    avg_loss = total_loss / total_samples if total_samples > 0 else 0\n",
    "    accuracy = correct / total_samples if total_samples > 0 else 0\n",
    "    \n",
    "    print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {avg_loss:>8f} \\n\")\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6156fd0d-e24b-4070-bc1d-b5b02532d22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  ..., 31, 31, 31])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer = SummaryWriter('runs/signature_gnn')\n",
    "\n",
    "input_dim = train_graph[0].x.shape[1]\n",
    "hidden_dim = 64\n",
    "output_dim = 128\n",
    "\n",
    "model = SignatureGCN(input_dim, hidden_dim, output_dim).to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loader = DataLoader(train_graph, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_graph, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f27ffd2-0e9e-4648-a104-d97d16b50199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                 | 0/92 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The `index` argument must be one-dimensional (got 2 dimensions)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     train_loss, train_accV \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m test_loop(val_graph, model, loss_fn)\n",
      "Cell \u001b[1;32mIn[50], line 14\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(graphs, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     11\u001b[0m graph_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m graph_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 14\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m embedding\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\documents\\machine_learning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\documents\\machine_learning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[42], line 19\u001b[0m, in \u001b[0;36mSignatureGCN.forward\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m     16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x, edge_index)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Aggregate node embeddings into a graph-level signature embedding\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mglobal_mean_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [num_graphs, embedding_dim]\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\documents\\machine_learning\\.venv\\lib\\site-packages\\torch_geometric\\nn\\pool\\glob.py:63\u001b[0m, in \u001b[0;36mglobal_mean_pool\u001b[1;34m(x, batch, size)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39mdim, keepdim\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\documents\\machine_learning\\.venv\\lib\\site-packages\\torch_geometric\\utils\\_scatter.py:43\u001b[0m, in \u001b[0;36mscatter\u001b[1;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Reduces all values from the :obj:`src` tensor at the indices\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03mspecified in the :obj:`index` tensor along a given dimension\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m:obj:`dim`. See the `documentation\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m        :obj:`\"any\"`). (default: :obj:`\"sum\"`)\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, Tensor) \u001b[38;5;129;01mand\u001b[39;00m index\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `index` argument must be one-dimensional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     44\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m dim \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m+\u001b[39m dim \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m dim\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(src, Tensor) \u001b[38;5;129;01mand\u001b[39;00m (dim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dim \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mdim()):\n",
      "\u001b[1;31mValueError\u001b[0m: The `index` argument must be one-dimensional (got 2 dimensions)"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Training\n",
    "    train_loss, train_accV = train_loop(train_graph, model, loss_fn, optimizer)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc = test_loop(val_graph, model, loss_fn)\n",
    "\n",
    "    # Logging\n",
    "    writer.add_scalar('Loss/Train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "    writer.add_scalar('Accuracy/Train', train_acc, epoch)\n",
    "    writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_signature_model.pth')\n",
    "        print(f\"New best model saved with validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "writer.close()\n",
    "print(f\"\\nTraining completed! Best validation accuracy: {best_val_acc:.4f}\")\n",
    "print(\"Model saved as 'best_signature_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78c75a9-a718-4931-a566-0fe071c1d261",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f3573-6944-4c4c-921e-3a3444fd10d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
