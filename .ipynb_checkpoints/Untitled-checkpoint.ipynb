{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09058c38-313b-4c55-aa68-0fc0bfe0e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn import GCNConv, VGAE\n",
    "\n",
    "import os\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib import tmap\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from lib.lib import SiameseSignatureDataset, image_to_graph\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e633d686-b2f4-45dd-a27e-27905a3de2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "w_d = 1e-5\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f46a5f2c-bfa4-470d-8c68-7abf9e00b273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/mallapraveen/signature-matching?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 468M/468M [01:30<00:00, 5.42MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 85246 signature images (genuine + forged)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "def dataset_path():\n",
    "    path = kagglehub.dataset_download(\"mallapraveen/signature-matching\")\n",
    "    return os.path.join(path, 'custom\\\\full')\n",
    "\n",
    "def transform(**kwargs):\n",
    "    return transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=kwargs['num_output_channels']),\n",
    "        transforms.Resize(kwargs['resize']),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "dataset = SiameseSignatureDataset(\n",
    "    root_dir=dataset_path(),\n",
    "    signer_folders=df,\n",
    "    transform=transform(num_output_channels=1, resize=(150, 150))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37aa2f5f-9089-48c1-afa2-8f1cb09d8688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes - Train: 68196, Validation: 17050\n"
     ]
    }
   ],
   "source": [
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "print(f\"Dataset sizes - Train: {train_size}, Validation: {val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8300287-5d08-4c2a-86c7-b03099087616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[1024, 3], edge_index=[2, 3968]),\n",
       " Data(x=[1024, 3], edge_index=[2, 3968]),\n",
       " 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "426eb5a9-883a-4a5f-a079-b77c9dfc9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbb5cf91-6448-4e67-8f1b-74b92f7642d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataBatch(x=[16384, 3], edge_index=[2, 63488], batch=[16384], ptr=[17]),\n",
       " DataBatch(x=[16384, 3], edge_index=[2, 63488], batch=[16384], ptr=[17]),\n",
       " tensor([0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b201d4-7a90-4576-943b-acc5abd668af",
   "metadata": {},
   "source": [
    "## Above is the data preperation\n",
    "# Now let's proceed to the creation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7023f923-840c-4848-a1b2-a74211b791e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, latent_dim):\n",
    "        super(GNNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv_mu = GCNConv(hidden_channels, latent_dim)\n",
    "        self.conv_logvar = GCNConv(hidden_channels, latent_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Step 1: Aggregate node features from neighbors\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "\n",
    "        # Step 2: Output mean and log variance\n",
    "        mu = self.conv_mu(x, edge_index)\n",
    "        logvar = self.conv_logvar(x, edge_index)\n",
    "\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb3b13d6-b30f-4cc8-b7d4-ff237615b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveSiameseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Siamese Network using Contrastive Loss\n",
    "    Better for signature verification with distance-based similarity\n",
    "    \"\"\"\n",
    "    def __init__(self, gnn_vae_model, latent_dim, margin=2.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gnn_vae = gnn_vae_model\n",
    "        self.latent_dim = latent_dim\n",
    "        self.margin = margin  # Margin for contrastive loss\n",
    "        \n",
    "        # Freeze GNN-VAE\n",
    "        for param in self.gnn_vae.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Optional: Additional projection layer\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "    \n",
    "    def forward_one(self, x, edge_index):\n",
    "        \"\"\"Extract and project features\"\"\"\n",
    "        with torch.no_grad():\n",
    "            embedding = self.gnn_vae.encode(x, edge_index)\n",
    "        \n",
    "        # Optional projection\n",
    "        embedding = self.projection(embedding)\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def forward(self, x1, edge_index1, x2, edge_index2):\n",
    "        \"\"\"\n",
    "        Returns embeddings and Euclidean distance\n",
    "        \"\"\"\n",
    "        emb1 = self.forward_one(x1, edge_index1)\n",
    "        emb2 = self.forward_one(x2, edge_index2)\n",
    "        \n",
    "        # Compute Euclidean distance\n",
    "        distance = F.pairwise_distance(emb1, emb2)\n",
    "        \n",
    "        return distance, emb1, emb2\n",
    "    \n",
    "    def predict(self, x1, edge_index1, x2, edge_index2, threshold=1.0):\n",
    "        \"\"\"\n",
    "        Predict based on distance threshold\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            distance, _, _ = self.forward(x1, edge_index1, x2, edge_index2)\n",
    "            is_same_person = distance < threshold\n",
    "            return is_same_person.item(), distance.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "347a5856-cd10-470c-8f14-4ec1c2b78049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(distance, label, margin=2.0):\n",
    "    \"\"\"\n",
    "    Contrastive loss for Siamese networks\n",
    "    \n",
    "    Args:\n",
    "        distance: Euclidean distance between embeddings\n",
    "        label: 1 if same person, 0 if different\n",
    "        margin: Margin for negative pairs\n",
    "    \n",
    "    Returns:\n",
    "        loss: Contrastive loss value\n",
    "    \"\"\"\n",
    "    loss = torch.mean(\n",
    "        label * torch.pow(distance, 2) +  # Same person: minimize distance\n",
    "        (1 - label) * torch.pow(torch.clamp(margin - distance, min=0.0), 2)  # Different: maximize distance\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7490e2c-49c4-49c8-88e6-b1761463947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1, _, _ = next(iter(train_loader))\n",
    "\n",
    "input_dim = img1.x.shape[1]\n",
    "hidden_dim = 64\n",
    "latent_dim = 128\n",
    "# epochs = 500\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "938a4ae4-1556-4157-a7b0-6b8a10e42dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGAE(\n",
       "  (encoder): GNNEncoder(\n",
       "    (conv1): GCNConv(3, 64)\n",
       "    (conv_mu): GCNConv(64, 128)\n",
       "    (conv_logvar): GCNConv(64, 128)\n",
       "  )\n",
       "  (decoder): InnerProductDecoder()\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your trained GNN-VAE\n",
    "checkpoint = torch.load('VGAE_Model.pt')\n",
    "vgae = VGAE(GNNEncoder(in_channels=input_dim, hidden_channels=hidden_dim, latent_dim=latent_dim))\n",
    "vgae.load_state_dict(checkpoint)\n",
    "vgae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cde1ec3c-7e76-40a3-9521-11eb0c413503",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_model = ContrastiveSiameseNetwork(\n",
    "    gnn_vae_model=vgae,\n",
    "    latent_dim=128,\n",
    "    margin=2.0\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a3a70f6-ed0b-481b-a196-e44ae7e372e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_loader, criterion, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch1, batch2, labels in tqdm(train_loader):\n",
    "        batch1 = batch1.to(device)\n",
    "        batch2 = batch2.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        similarity, emb1, emb2 = model(\n",
    "            batch1.x, batch1.edge_index,\n",
    "            batch2.x, batch2.edge_index\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(similarity, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Collect predictions\n",
    "        predictions = (similarity > 0.5).float()\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Compute metrics\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_similarities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch1, batch2, labels in tqdm(val_loader):\n",
    "            batch1 = batch1.to(device)\n",
    "            batch2 = batch2.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            # Forward pass\n",
    "            similarity, emb1, emb2 = model(\n",
    "                batch1.x, batch1.edge_index,\n",
    "                batch2.x, batch2.edge_index\n",
    "            )\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(similarity, labels)\n",
    "            \n",
    "            # Collect predictions\n",
    "            predictions = (similarity > 0.5).float()\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_similarities.extend(similarity.cpu().numpy())\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    # Compute metrics\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
    "    \n",
    "    # AUC-ROC\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_similarities)\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    metrics = {\n",
    "        'val_loss': avg_loss,\n",
    "        'val_accuracy': accuracy,\n",
    "        'val_precision': precision,\n",
    "        'val_recall': recall,\n",
    "        'val_f1': f1,\n",
    "        'val_auc': auc\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea5f213c-5af6-4c92-aaa0-6820db800a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = contrastive_loss\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.Adam(\n",
    "    contrastive_model.parameters(),\n",
    "    lr=0.0001,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    # verbose=True\n",
    ")\n",
    "\n",
    "# Training settings\n",
    "epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "best_val_f1 = 0.0\n",
    "patience = 10\n",
    "patience_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "42bc7960-703c-466c-b96e-4fbced9753d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING SIAMESE NETWORK\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 4263/4263 [59:42<00:00,  1.19it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [68196, 69832704]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontrastive_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     val_metrics \u001b[38;5;241m=\u001b[39m validate_epoch(\n\u001b[0;32m     24\u001b[0m         contrastive_model,\n\u001b[0;32m     25\u001b[0m         val_loader,\n\u001b[0;32m     26\u001b[0m         criterion,\n\u001b[0;32m     27\u001b[0m         device\n\u001b[0;32m     28\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[23], line 37\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, optimizer, train_loader, criterion, device)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Compute metrics\u001b[39;00m\n\u001b[0;32m     36\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m---> 37\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_predictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m avg_loss, accuracy\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\projects\\asrs\\machine-learning\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\projects\\asrs\\machine-learning\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:227\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m    226\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m attach_unique(y_true, y_pred)\n\u001b[1;32m--> 227\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    228\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\projects\\asrs\\machine-learning\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:98\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \n\u001b[0;32m     73\u001b[0m \u001b[38;5;124;03mThis converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;124;03my_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     97\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y_true, y_pred)\n\u001b[1;32m---> 98\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\projects\\asrs\\machine-learning\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    478\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [68196, 69832704]"
     ]
    }
   ],
   "source": [
    "# TensorBoard (optional)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "writer = SummaryWriter(f'runs/siamese_{timestamp}')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SIAMESE NETWORK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Training\n",
    "    train_loss, train_accuracy = train_epoch(\n",
    "        contrastive_model,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        criterion,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    val_metrics = validate_epoch(\n",
    "        contrastive_model,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_metrics['val_loss'])\n",
    "    \n",
    "    # Logging to tensorboard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', val_metrics['val_loss'], epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_metrics['val_accuracy'], epoch)\n",
    "    writer.add_scalar('Metrics/precision', val_metrics['val_precision'], epoch)\n",
    "    writer.add_scalar('Metrics/recall', val_metrics['val_recall'], epoch)\n",
    "    writer.add_scalar('Metrics/f1', val_metrics['val_f1'], epoch)\n",
    "    writer.add_scalar('Metrics/auc', val_metrics['val_auc'], epoch)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"\\nEpoch {epoch:03d}/{epochs}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.4f}\")\n",
    "    print(f\"  Val Loss:   {val_metrics['val_loss']:.4f} | Val Acc:  {val_metrics['val_accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {val_metrics['val_precision']:.4f} | Recall: {val_metrics['val_recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {val_metrics['val_f1']:.4f} | AUC:    {val_metrics['val_auc']:.4f}\")\n",
    "    \n",
    "    # Save best model based on validation F1 score\n",
    "    if val_metrics['val_f1'] > best_val_f1:\n",
    "        best_val_f1 = val_metrics['val_f1']\n",
    "        best_val_loss = val_metrics['val_loss']\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': siamese_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_metrics['val_loss'],\n",
    "            'val_f1': val_metrics['val_f1'],\n",
    "            'val_metrics': val_metrics\n",
    "        }, f'best_siamese_{timestamp}.pt')\n",
    "        \n",
    "        print(f\"  ✓ Best model saved! F1: {best_val_f1:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best Validation F1 Score: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109956e4-4907-466c-a4be-dde52975fda3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
