{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09058c38-313b-4c55-aa68-0fc0bfe0e9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Batch\n",
    "from torch_geometric.nn import GCNConv, VGAE\n",
    "\n",
    "import os\n",
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib import tmap\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from lib.lib import SiameseSignatureDataset, image_to_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e633d686-b2f4-45dd-a27e-27905a3de2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "w_d = 1e-5\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f46a5f2c-bfa4-470d-8c68-7abf9e00b273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 85246 signature images (genuine + forged)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "def dataset_path():\n",
    "    path = kagglehub.dataset_download(\"mallapraveen/signature-matching\")\n",
    "    return os.path.join(path, 'custom\\\\full')\n",
    "\n",
    "def transform(**kwargs):\n",
    "    return transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=kwargs['num_output_channels']),\n",
    "        transforms.Resize(kwargs['resize']),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "dataset = SiameseSignatureDataset(\n",
    "    root_dir=dataset_path(),\n",
    "    signer_folders=df,\n",
    "    transform=transform(num_output_channels=1, resize=(150, 150))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37aa2f5f-9089-48c1-afa2-8f1cb09d8688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes - Train: 68196, Validation: 17050\n"
     ]
    }
   ],
   "source": [
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "print(f\"Dataset sizes - Train: {train_size}, Validation: {val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8300287-5d08-4c2a-86c7-b03099087616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[1024, 3], edge_index=[2, 3968]),\n",
       " Data(x=[1024, 3], edge_index=[2, 3968]),\n",
       " 0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "426eb5a9-883a-4a5f-a079-b77c9dfc9723",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bbb5cf91-6448-4e67-8f1b-74b92f7642d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataBatch(x=[16384, 3], edge_index=[2, 63488], batch=[16384], ptr=[17]),\n",
       " DataBatch(x=[16384, 3], edge_index=[2, 63488], batch=[16384], ptr=[17]),\n",
       " tensor([1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0])]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b201d4-7a90-4576-943b-acc5abd668af",
   "metadata": {},
   "source": [
    "## Above is the data preperation\n",
    "# Now let's proceed to the creation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7023f923-840c-4848-a1b2-a74211b791e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, latent_dim):\n",
    "        super(GNNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv_mu = GCNConv(hidden_channels, latent_dim)\n",
    "        self.conv_logvar = GCNConv(hidden_channels, latent_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Step 1: Aggregate node features from neighbors\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "\n",
    "        # Step 2: Output mean and log variance\n",
    "        mu = self.conv_mu(x, edge_index, batch)\n",
    "        logvar = self.conv_logvar(x, edge_index, batch)\n",
    "\n",
    "        return mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb3b13d6-b30f-4cc8-b7d4-ff237615b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveSiameseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Siamese Network using Contrastive Loss\n",
    "    Better for signature verification with distance-based similarity\n",
    "    \"\"\"\n",
    "    def __init__(self, gnn_vae_model, latent_dim=32, margin=2.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gnn_vae = gnn_vae_model\n",
    "        self.latent_dim = latent_dim\n",
    "        self.margin = margin  # Margin for contrastive loss\n",
    "        \n",
    "        # Freeze GNN-VAE\n",
    "        for param in self.gnn_vae.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Optional: Additional projection layer\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "    \n",
    "    def forward_one(self, x, edge_index, batch=None):\n",
    "        \"\"\"Extract and project features\"\"\"\n",
    "        with torch.no_grad():\n",
    "            embedding = self.gnn_vae.encode(x, edge_index, batch)\n",
    "        \n",
    "        # Optional projection\n",
    "        embedding = self.projection(embedding)\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    def forward(self, x1, edge_index1, x2, edge_index2, batch1=None, batch2=None):\n",
    "        \"\"\"\n",
    "        Returns embeddings and Euclidean distance\n",
    "        \"\"\"\n",
    "        emb1 = self.forward_one(x1, edge_index1, batch1)\n",
    "        emb2 = self.forward_one(x2, edge_index2, batch2)\n",
    "        \n",
    "        # Compute Euclidean distance\n",
    "        distance = F.pairwise_distance(emb1, emb2)\n",
    "        \n",
    "        return distance, emb1, emb2\n",
    "    \n",
    "    def predict(self, x1, edge_index1, x2, edge_index2, threshold=1.0):\n",
    "        \"\"\"\n",
    "        Predict based on distance threshold\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            distance, _, _ = self.forward(x1, edge_index1, x2, edge_index2)\n",
    "            is_same_person = distance < threshold\n",
    "            return is_same_person.item(), distance.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "347a5856-cd10-470c-8f14-4ec1c2b78049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(distance, label, margin=2.0):\n",
    "    \"\"\"\n",
    "    Contrastive loss for Siamese networks\n",
    "    \n",
    "    Args:\n",
    "        distance: Euclidean distance between embeddings\n",
    "        label: 1 if same person, 0 if different\n",
    "        margin: Margin for negative pairs\n",
    "    \n",
    "    Returns:\n",
    "        loss: Contrastive loss value\n",
    "    \"\"\"\n",
    "    loss = torch.mean(\n",
    "        label * torch.pow(distance, 2) +  # Same person: minimize distance\n",
    "        (1 - label) * torch.pow(torch.clamp(margin - distance, min=0.0), 2)  # Different: maximize distance\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d7490e2c-49c4-49c8-88e6-b1761463947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1, _, _ = next(iter(train_loader))\n",
    "\n",
    "input_dim = img1.x.shape[1]\n",
    "hidden_dim = 64\n",
    "latent_dim = 128\n",
    "# epochs = 500\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "938a4ae4-1556-4157-a7b0-6b8a10e42dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGAE(\n",
       "  (encoder): GNNEncoder(\n",
       "    (conv1): GCNConv(3, 64)\n",
       "    (conv_mu): GCNConv(64, 128)\n",
       "    (conv_logvar): GCNConv(64, 128)\n",
       "  )\n",
       "  (decoder): InnerProductDecoder()\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your trained GNN-VAE\n",
    "checkpoint = torch.load('VGAE_Model.pt')\n",
    "vgae = VGAE(GNNEncoder(in_channels=input_dim, hidden_channels=hidden_dim, latent_dim=latent_dim))\n",
    "vgae.load_state_dict(checkpoint)\n",
    "vgae.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cde1ec3c-7e76-40a3-9521-11eb0c413503",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_model = ContrastiveSiameseNetwork(\n",
    "    gnn_vae_model=vgae,\n",
    "    latent_dim=128,\n",
    "    margin=2.0\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a3a70f6-ed0b-481b-a196-e44ae7e372e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, train_loader, criterion, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch1, batch2, labels in train_loader:\n",
    "        batch1 = batch1.to(device)\n",
    "        batch2 = batch2.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        similarity, emb1, emb2 = model(\n",
    "            batch1.x, batch1.edge_index,\n",
    "            batch2.x, batch2.edge_index,\n",
    "            batch1.batch, batch2.batch\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(similarity, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Collect predictions\n",
    "        predictions = (similarity > 0.5).float()\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Compute metrics\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate_epoch(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_similarities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch1, batch2, labels in val_loader:\n",
    "            batch1 = batch1.to(device)\n",
    "            batch2 = batch2.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            # Forward pass\n",
    "            similarity, emb1, emb2 = model(\n",
    "                batch1.x, batch1.edge_index,\n",
    "                batch2.x, batch2.edge_index,\n",
    "                batch1.batch, batch2.batch\n",
    "            )\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(similarity, labels)\n",
    "            \n",
    "            # Collect predictions\n",
    "            predictions = (similarity > 0.5).float()\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_similarities.extend(similarity.cpu().numpy())\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    # Compute metrics\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_predictions, zero_division=0)\n",
    "    \n",
    "    # AUC-ROC\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_similarities)\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    metrics = {\n",
    "        'val_loss': avg_loss,\n",
    "        'val_accuracy': accuracy,\n",
    "        'val_precision': precision,\n",
    "        'val_recall': recall,\n",
    "        'val_f1': f1,\n",
    "        'val_auc': auc\n",
    "    }\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ea5f213c-5af6-4c92-aaa0-6820db800a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING SIAMESE NETWORK\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "GNNEncoder.forward() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Training loop\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     train_loss, train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontrastive_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     val_metrics \u001b[38;5;241m=\u001b[39m validate_epoch(\n\u001b[0;32m     49\u001b[0m         contrastive_model,\n\u001b[0;32m     50\u001b[0m         val_loader,\n\u001b[0;32m     51\u001b[0m         criterion,\n\u001b[0;32m     52\u001b[0m         device\n\u001b[0;32m     53\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[52], line 17\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, optimizer, train_loader, criterion, device)\u001b[0m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m similarity, emb1, emb2 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(similarity, labels)\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\projects\\asrs\\machine-learning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\projects\\asrs\\machine-learning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[47], line 38\u001b[0m, in \u001b[0;36mContrastiveSiameseNetwork.forward\u001b[1;34m(self, x1, edge_index1, x2, edge_index2, batch1, batch2)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x1, edge_index1, x2, edge_index2, batch1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, batch2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    Returns embeddings and Euclidean distance\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     emb1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     emb2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_one(x2, edge_index2, batch2)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Compute Euclidean distance\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[47], line 27\u001b[0m, in \u001b[0;36mContrastiveSiameseNetwork.forward_one\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Extract and project features\"\"\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 27\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn_vae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Optional projection\u001b[39;00m\n\u001b[0;32m     30\u001b[0m embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(embedding)\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\projects\\asrs\\machine-learning\\.venv\\lib\\site-packages\\torch_geometric\\nn\\models\\autoencoder.py:169\u001b[0m, in \u001b[0;36mVGAE.encode\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    168\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__mu__, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__logstd__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__logstd__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__logstd__\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mMAX_LOGSTD)\n\u001b[0;32m    171\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparametrize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__mu__, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__logstd__)\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\projects\\asrs\\machine-learning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\projects\\asrs\\machine-learning\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mTypeError\u001b[0m: GNNEncoder.forward() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "criterion = contrastive_loss\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.Adam(\n",
    "    contrastive_model.parameters(),\n",
    "    lr=0.0001,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    # verbose=True\n",
    ")\n",
    "\n",
    "# Training settings\n",
    "epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "best_val_f1 = 0.0\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "\n",
    "# TensorBoard (optional)\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "writer = SummaryWriter(f'runs/siamese_{timestamp}')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SIAMESE NETWORK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Training\n",
    "    train_loss, train_accuracy = train_epoch(\n",
    "        contrastive_model,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        criterion,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    # Validation\n",
    "    val_metrics = validate_epoch(\n",
    "        contrastive_model,\n",
    "        val_loader,\n",
    "        criterion,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_metrics['val_loss'])\n",
    "    \n",
    "    # Logging to tensorboard\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', val_metrics['val_loss'], epoch)\n",
    "    writer.add_scalar('Accuracy/train', train_accuracy, epoch)\n",
    "    writer.add_scalar('Accuracy/val', val_metrics['val_accuracy'], epoch)\n",
    "    writer.add_scalar('Metrics/precision', val_metrics['val_precision'], epoch)\n",
    "    writer.add_scalar('Metrics/recall', val_metrics['val_recall'], epoch)\n",
    "    writer.add_scalar('Metrics/f1', val_metrics['val_f1'], epoch)\n",
    "    writer.add_scalar('Metrics/auc', val_metrics['val_auc'], epoch)\n",
    "    \n",
    "    # Print progress\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"\\nEpoch {epoch:03d}/{epochs}\")\n",
    "        print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_accuracy:.4f}\")\n",
    "        print(f\"  Val Loss:   {val_metrics['val_loss']:.4f} | Val Acc:  {val_metrics['val_accuracy']:.4f}\")\n",
    "        print(f\"  Precision: {val_metrics['val_precision']:.4f} | Recall: {val_metrics['val_recall']:.4f}\")\n",
    "        print(f\"  F1 Score:  {val_metrics['val_f1']:.4f} | AUC:    {val_metrics['val_auc']:.4f}\")\n",
    "    \n",
    "    # Save best model based on validation F1 score\n",
    "    if val_metrics['val_f1'] > best_val_f1:\n",
    "        best_val_f1 = val_metrics['val_f1']\n",
    "        best_val_loss = val_metrics['val_loss']\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save checkpoint\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': siamese_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_metrics['val_loss'],\n",
    "            'val_f1': val_metrics['val_f1'],\n",
    "            'val_metrics': val_metrics\n",
    "        }, f'best_siamese_{timestamp}.pt')\n",
    "        \n",
    "        print(f\"  ✓ Best model saved! F1: {best_val_f1:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "writer.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ TRAINING COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "print(f\"Best Validation F1 Score: {best_val_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc7960-703c-466c-b96e-4fbced9753d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
