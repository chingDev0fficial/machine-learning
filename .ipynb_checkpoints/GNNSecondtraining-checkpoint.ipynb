{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d4378e1-7d15-4e9b-81d7-4db343171e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import image\n",
    "import cv2\n",
    "from typing import Tuple, Optional, Union\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "from lib.lib import SignatureDataset, image_to_graph\n",
    "from torch_geometric.nn import GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e11c622-4da3-42c4-b46f-1374c9b1d97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32  # Changed to match your DataLoader batch_size\n",
    "epochs = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65c3b71c-8153-466d-897b-186ee7ff5720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureGNN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, embedding_dim)\n",
    "        \n",
    "    def forward(self, x: Tensor, edge_index: Tensor, batch: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: Node features [num_nodes, in_channels]\n",
    "        edge_index: Graph edges [2, num_edges]\n",
    "        batch: Graph IDs for mini-batch training [num_nodes] - not used for GAE\n",
    "        \"\"\"\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # Return node-level embeddings for GAE\n",
    "        # Do NOT use global_mean_pool for GAE - it needs node embeddings\n",
    "        return x  # [num_nodes, embedding_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cc59917-7cf2-4c5c-9279-91571d872a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 14626 signature images (genuine + forged)\n"
     ]
    }
   ],
   "source": [
    "def dataset_path():\n",
    "    path = kagglehub.dataset_download(\"akashgundu/signature-verification-dataset\")\n",
    "    return os.path.join(path, 'extract')\n",
    "\n",
    "def transform(**kwargs):\n",
    "    return transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=kwargs['num_output_channels']),\n",
    "        transforms.Resize(kwargs['resize']),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "dataset = SignatureDataset(\n",
    "    root_dir=dataset_path(),\n",
    "    transform=transform(num_output_channels=1, resize=(150, 150))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "939c43dd-25c0-4ad4-8e17-71bc36e6ac39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes - Train: 11700, Validation: 2926\n"
     ]
    }
   ],
   "source": [
    "total_size = len(dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "val_size = total_size - train_size\n",
    "train_dataset, val_dataset = random_split(\n",
    "    dataset,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "print(f\"Dataset sizes - Train: {train_size}, Validation: {val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75f03342-a546-4cf0-abc7-fc1f970d3b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Graphs: 100%|██████████████████████████████████████████████████████████████| 11700/11700 [07:43<00:00, 25.27it/s]\n",
      "Val Graphs: 100%|██████████████████████████████████████████████████████████████████| 2926/2926 [02:11<00:00, 22.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train graphs: 11700\n",
      "Val graphs: 2926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_graph = []\n",
    "val_graph = []\n",
    "\n",
    "# Convert training dataset\n",
    "for t in tqdm(train_dataset, desc=\"Train Graphs\"):\n",
    "    for train_tensor_image in t:\n",
    "        t_graph = image_to_graph(train_tensor_image)\n",
    "        train_graph.append(t_graph)\n",
    "\n",
    "# Convert validation dataset\n",
    "for v in tqdm(val_dataset, desc=\"Val Graphs\"):\n",
    "    for val_tensor_image in v:\n",
    "        v_graph = image_to_graph(val_tensor_image)\n",
    "        val_graph.append(v_graph)\n",
    "\n",
    "print(\"Train graphs:\", len(train_graph))\n",
    "print(\"Val graphs:\", len(val_graph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d10f8dec-a373-4496-8c44-76ab6edc5d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from datetime import datetime\n",
    "\n",
    "# Create unique writer for each run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "writer = SummaryWriter(f'runs/gae_experiment_{timestamp}')\n",
    "\n",
    "def train_loop(model, graphs, val_graphs, lr, epochs=50, batch=None):\n",
    "    model = model.to(device)  # move model to GPU/CPU\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for graph in tqdm(graphs):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Move graph data to device\n",
    "            x, edge_index = graph.x.to(device), graph.edge_index.to(device)\n",
    "\n",
    "            z = model.encode(x, edge_index)\n",
    "            loss = model.recon_loss(z, edge_index)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(graphs)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_val_loss, total_auc, total_ap = 0, 0, 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_graph in tqdm(val_graphs):\n",
    "                x, edge_index = val_graph.x.to(device), val_graph.edge_index.to(device)\n",
    "\n",
    "                z = model.encode(x, edge_index)\n",
    "                val_loss = model.recon_loss(z, edge_index)\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                # Negative sampling\n",
    "                pos_edge_index = edge_index\n",
    "                neg_edge_index = negative_sampling(\n",
    "                    pos_edge_index, \n",
    "                    num_nodes=x.size(0),\n",
    "                    num_neg_samples=pos_edge_index.size(1)\n",
    "                ).to(device)\n",
    "\n",
    "                # Predictions\n",
    "                pos_pred = model.decoder(z, pos_edge_index, sigmoid=True)\n",
    "                neg_pred = model.decoder(z, neg_edge_index, sigmoid=True)\n",
    "\n",
    "                # Labels + predictions\n",
    "                y_true = torch.cat([\n",
    "                    torch.ones(pos_pred.size(0), device=device),\n",
    "                    torch.zeros(neg_pred.size(0), device=device)\n",
    "                ])\n",
    "                y_pred = torch.cat([pos_pred, neg_pred])\n",
    "\n",
    "                # Metrics (move to CPU for sklearn)\n",
    "                auc = roc_auc_score(y_true.cpu(), y_pred.cpu())\n",
    "                ap = average_precision_score(y_true.cpu(), y_pred.cpu())\n",
    "                \n",
    "                total_auc += auc\n",
    "                total_ap += ap\n",
    "                \n",
    "        avg_val_loss = total_val_loss / len(val_graphs)\n",
    "        avg_auc = total_auc / len(val_graphs)\n",
    "        avg_ap = total_ap / len(val_graphs)\n",
    "        \n",
    "        # Log\n",
    "        writer.add_scalar('Accuracy/AUC', avg_auc, epoch)\n",
    "        writer.add_scalar('Accuracy/AP', avg_ap, epoch)\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Val', avg_val_loss, epoch)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, AUC: {avg_auc:.4f}, AP: {avg_ap:.4f}')\n",
    "    \n",
    "    writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93caecfc-668f-4ab1-9485-99e9ef880905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▎                                                                            | 203/11700 [00:09<09:09, 20.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'orch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 12\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m     gae \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[17], line 26\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(model, graphs, val_graphs, lr, epochs, batch)\u001b[0m\n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mrecon_loss(z, edge_index)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\Documents\\machine_learning\\.venv\\lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\machine_learning\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\machine_learning\\.venv\\lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving Model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m     \u001b[43morch\u001b[49m\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVAE_GNN_model.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'orch' is not defined"
     ]
    }
   ],
   "source": [
    "# from torch_geometric.nn import VGAE\n",
    "input_dim = next(iter(train_graph)).x.shape[1]\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "epochs = 500\n",
    "# epochs = 200\n",
    "\n",
    "encoder = SignatureGNN(input_dim, hidden_dim, embedding_dim)\n",
    "model = GAE(encoder)\n",
    "\n",
    "try:\n",
    "    gae = train_loop(model, train_graph, val_graph, learning_rate, epochs)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Saving Model...\")\n",
    "    torch.save(model.state_dict(), \"VAE_GNN_model.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5490e3-36c0-4f44-8576-0d06bc977944",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gae.state_dict(), \"VAE_GNN_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ef697-bc52-4f2c-a4cc-60bf6ed0acfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = next(iter(train_graph))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27d737-dd43-4998-81d8-be3357b22908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(**kwargs):\n",
    "    return transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=kwargs['num_output_channels']),\n",
    "        transforms.Resize(kwargs['resize']),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "dataset = SignatureDataset(\n",
    "    root_dir=\"test_image\",\n",
    "    transform=transform(num_output_channels=1, resize=(150, 150))\n",
    ")\n",
    "\n",
    "test_graph = []\n",
    "\n",
    "for t in dataset:\n",
    "    for test_tensor_image in t:\n",
    "        t_graph = image_to_graph(test_tensor_image)\n",
    "        test_graph.append(t_graph)\n",
    "\n",
    "graphs = DataLoader(test_graph, batch_size=batch_size, shuffle=False)\n",
    "print(len(test_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c4f7a8da-d4e8-49b4-ba4e-7446ea3e55c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SignatureGNN(input_dim, hidden_dim, embedding_dim)\n",
    "model = GAE(encoder)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = model.encode(sample.x, sample.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b9644c1b-0587-4843-b437-c483e581ab4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1388, -0.1934,  0.1532,  ...,  0.1042,  0.1565,  0.0883],\n",
       "        [ 0.1617, -0.2071,  0.1523,  ...,  0.1083,  0.1765,  0.0973],\n",
       "        [ 0.1558, -0.2031,  0.1537,  ...,  0.1067,  0.1720,  0.0956],\n",
       "        ...,\n",
       "        [-0.0241,  0.0962, -0.0785,  ..., -0.0100, -0.1637, -0.1377],\n",
       "        [ 0.0465,  0.0573, -0.0675,  ...,  0.0338, -0.0884, -0.1084],\n",
       "        [ 0.0884,  0.0128, -0.0262,  ...,  0.0715, -0.0210, -0.0687]])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d306473e-79f4-46d8-8a45-d4045dfe6424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
