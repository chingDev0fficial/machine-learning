{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f9ddfe9-d00f-40c9-93b7-a8617f3b9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torchvision import transforms\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import image\n",
    "import cv2\n",
    "from typing import Tuple, Optional, Union\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0c98771-7c8f-43e5-b5b3-bb4c231fb992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureGCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, embedding_dim)\n",
    "        \n",
    "    def forward(self, x: Tensor, edge_index: Tensor, batch: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        x: Node features [num_nodes, in_channels]\n",
    "        edge_index: Graph edges [2, num_edges]\n",
    "        batch: Graph IDs for mini-batch training [num_nodes]\n",
    "        \"\"\"\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = self.conv2(x, edge_index).relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        \n",
    "        # Aggregate node embeddings into a graph-level signature embedding\n",
    "        x = global_mean_pool(x, batch)  # [num_graphs, embedding_dim]\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e52f2a-5523-4645-983e-ba18f2145ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SignatureGCN(\n",
       "  (conv1): GCNConv(4, 64)\n",
       "  (conv2): GCNConv(64, 64)\n",
       "  (conv3): GCNConv(64, 128)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your trained model\n",
    "model = SignatureGCN(in_channels=4, hidden_channels=64, embedding_dim=128)\n",
    "model.load_state_dict(torch.load('best_feature_extraction_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd09b0b-c45c-4638-9177-5f33a9ea7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignatureDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        # collect all signer folders\n",
    "        signer_folders = sorted(os.listdir(root_dir))\n",
    "\n",
    "        for folder in signer_folders:\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "            if os.path.isdir(folder_path):\n",
    "                for img_name in os.listdir(folder_path):\n",
    "                    if self._is_image_file(img_name):\n",
    "                        self.samples.append(os.path.join(folder_path, img_name))\n",
    "\n",
    "        print(f\"Loaded {len(self.samples)} signature images (genuine + forged)\")\n",
    "\n",
    "    def _is_image_file(self, filename):\n",
    "        valid_exts = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "        return os.path.splitext(filename.lower())[1] in valid_exts\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(path).convert(\"L\")  # grayscale\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image   # only image, no label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {path}: {e}\")\n",
    "            # fallback blank image\n",
    "            fallback = Image.new(\"L\", (224, 224), 0)\n",
    "            if self.transform:\n",
    "                fallback = self.transform(fallback)\n",
    "            return fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3e9e35a-91aa-468e-b1a1-0b6d48823968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 signature images (genuine + forged)\n"
     ]
    }
   ],
   "source": [
    "def transform(**kwargs):\n",
    "    return transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=kwargs['num_output_channels']),\n",
    "        transforms.Resize(kwargs['resize']),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "dataset = SignatureDataset(\n",
    "    root_dir=\"test_image\",\n",
    "    transform=transform(num_output_channels=1, resize=(150, 150))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "990ceea1-8c6d-4283-a75e-cc1da21c80c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_graph(\n",
    "    image_tensor: torch.Tensor,\n",
    "    patch_size: int = 8,\n",
    "    k_neighbors: int = 8,\n",
    "    edge_threshold: float = 0.1,\n",
    "    include_features: bool = True\n",
    ") -> Data:\n",
    "    \"\"\"\n",
    "    Convert an image to a graph representation with nodes and edges.\n",
    "    \n",
    "    Args:\n",
    "        image_tensor: Input image tensor of shape (C, H, W) or (H, W)\n",
    "        method: Graph construction method ('grid', 'knn', 'superpixel', 'region')\n",
    "        patch_size: Size of patches for grid method\n",
    "        k_neighbors: Number of neighbors for KNN method\n",
    "        edge_threshold: Threshold for edge creation based on feature similarity\n",
    "        include_features: Whether to include patch features as node features\n",
    "        \n",
    "    Returns:\n",
    "        PyTorch Geometric Data object with node features and edge indices\n",
    "    \"\"\"\n",
    "    \n",
    "    return _image_to_grid_graph(image_tensor, patch_size, include_features)\n",
    "\n",
    "def _image_to_grid_graph(\n",
    "    image_tensor: torch.Tensor, \n",
    "    patch_size: int,\n",
    "    include_features: bool\n",
    ") -> Data:\n",
    "    \"\"\"Convert image to grid-based graph where each patch is a node.\"\"\"\n",
    "    \n",
    "    # Handle different input shapes\n",
    "    if len(image_tensor.shape) == 2:\n",
    "        image_tensor = image_tensor.unsqueeze(0)  # Add channel dimension\n",
    "    \n",
    "    C, H, W = image_tensor.shape\n",
    "    \n",
    "    # Create patches\n",
    "    patches_h = H // patch_size\n",
    "    patches_w = W // patch_size\n",
    "    \n",
    "    # Extract patch features\n",
    "    node_features = []\n",
    "    node_positions = []\n",
    "    \n",
    "    for i in range(patches_h):\n",
    "        for j in range(patches_w):\n",
    "            # Extract patch\n",
    "            patch = image_tensor[\n",
    "                :, \n",
    "                i * patch_size:(i + 1) * patch_size,\n",
    "                j * patch_size:(j + 1) * patch_size\n",
    "            ]\n",
    "            \n",
    "            if include_features:\n",
    "                # Compute patch statistics as features\n",
    "                mean_val = patch.mean(dim=[1, 2])  # Per channel mean\n",
    "                std_val = patch.std(dim=[1, 2])    # Per channel std\n",
    "                max_val = patch.max(dim=2)[0].max(dim=1)[0]  # Per channel max\n",
    "                min_val = patch.min(dim=2)[0].min(dim=1)[0]  # Per channel min\n",
    "                \n",
    "                features = torch.cat([mean_val, std_val, max_val, min_val])\n",
    "                node_features.append(features)\n",
    "            \n",
    "            # Store position\n",
    "            node_positions.append([i, j])\n",
    "    \n",
    "    # Create edges (connect adjacent patches)\n",
    "    edge_indices = []\n",
    "    \n",
    "    for i in range(patches_h):\n",
    "        for j in range(patches_w):\n",
    "            current_node = i * patches_w + j\n",
    "            \n",
    "            # Connect to neighbors (4-connectivity)\n",
    "            neighbors = [\n",
    "                (i-1, j), (i+1, j),  # vertical neighbors\n",
    "                (i, j-1), (i, j+1)   # horizontal neighbors\n",
    "            ]\n",
    "            \n",
    "            # Add diagonal connections for 8-connectivity\n",
    "            neighbors.extend([\n",
    "                (i-1, j-1), (i-1, j+1),\n",
    "                (i+1, j-1), (i+1, j+1)\n",
    "            ])\n",
    "            \n",
    "            for ni, nj in neighbors:\n",
    "                if 0 <= ni < patches_h and 0 <= nj < patches_w:\n",
    "                    neighbor_node = ni * patches_w + nj\n",
    "                    edge_indices.append([current_node, neighbor_node])\n",
    "    \n",
    "    # Convert to tensors\n",
    "    if include_features:\n",
    "        x = torch.stack(node_features)\n",
    "    else:\n",
    "        x = torch.tensor(node_positions, dtype=torch.float32)\n",
    "    \n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "    pos = torch.tensor(node_positions, dtype=torch.float32)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, pos=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b4bb4b-c4ad-4207-a2db-7359078547e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00,  4.69it/s]\n"
     ]
    }
   ],
   "source": [
    "graphs = []\n",
    "\n",
    "for image in tqdm(dataset):\n",
    "    for img in image:\n",
    "        graph = image_to_graph(img)\n",
    "        graphs.append(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a82191a1-1ad8-4aa3-9943-87ea00255d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[972, 4], edge_index=[2, 7140], pos=[972, 2], batch=[972], ptr=[4])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_graph = DataLoader(graphs, batch_size=32, shuffle=False)\n",
    "next(iter(my_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be046db7-5099-416f-af9f-2ce1385e6339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from new graphs\n",
    "features = []\n",
    "with torch.no_grad():\n",
    "    for graph in my_graph:\n",
    "        feature = model(graph.x, graph.edge_index, graph.batch)\n",
    "        features.append(feature.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "562e6509-0220-487a-ad14-d12588247c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Siamese Network for comparing image feature vectors\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=128, hidden_dims=[256, 128, 64], dropout_rate=0.3):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        # Shared feature processing network\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        self.feature_processor = nn.Sequential(*layers)\n",
    "        \n",
    "        # Final similarity computation layers\n",
    "        self.similarity_head = nn.Sequential(\n",
    "            nn.Linear(prev_dim * 2, 64),  # Concatenated features\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Output probability of match\n",
    "        )\n",
    "        \n",
    "        # Alternative: Distance-based approach\n",
    "        self.distance_head = nn.Sequential(\n",
    "            nn.Linear(prev_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        \"\"\"Process single feature vector through shared network\"\"\"\n",
    "        return self.feature_processor(x)\n",
    "    \n",
    "    def forward(self, x1, x2, use_distance=False):\n",
    "        \"\"\"\n",
    "        Forward pass for pair of feature vectors\n",
    "        Args:\n",
    "            x1, x2: Feature vectors of shape (batch_size, input_dim)\n",
    "            use_distance: If True, use distance-based similarity, else concatenation\n",
    "        \"\"\"\n",
    "        # Process both inputs through shared network\n",
    "        feat1 = self.forward_one(x1)\n",
    "        feat2 = self.forward_one(x2)\n",
    "        \n",
    "        if use_distance:\n",
    "            # Distance-based approach\n",
    "            distance = torch.abs(feat1 - feat2)\n",
    "            similarity = torch.exp(-self.distance_head(distance))\n",
    "            return similarity\n",
    "        else:\n",
    "            # Concatenation-based approach\n",
    "            combined = torch.cat([feat1, feat2], dim=1)\n",
    "            similarity = self.similarity_head(combined)\n",
    "            return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65ea0e88-bbb9-4b38-8c1e-cb0044bd3738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SiameseNetwork(\n",
       "  (feature_processor): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Dropout(p=0.3, inplace=False)\n",
       "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Dropout(p=0.3, inplace=False)\n",
       "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (similarity_head): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (6): Sigmoid()\n",
       "  )\n",
       "  (distance_head): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your trained model\n",
    "siamese_model = SiameseNetwork(input_dim=128)\n",
    "siamese_model.load_state_dict(torch.load('siamese_model_checkpoint.pth'))\n",
    "siamese_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08f5b3c4-bd03-4acc-baab-8a01c205eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to your images\n",
    "img1 = feature[1].unsqueeze(0)  # Add batch dimension\n",
    "img2 = feature[0].unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "with torch.no_grad():\n",
    "    result = siamese_model.forward(img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd4f631c-a0b0-4d80-9ca7-46440a7d6cf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.5227e-34]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "941dc45e-7f35-4e4a-9f1b-e2ab0853a719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0135,  0.0160, -0.0196,  0.0006, -0.0047, -0.0034,  0.0130, -0.0054,\n",
       "         -0.0260,  0.0066, -0.0148,  0.0079, -0.0225, -0.0065, -0.0007,  0.0103,\n",
       "         -0.0050,  0.0059, -0.0215, -0.0047,  0.0016,  0.0112, -0.0059, -0.0045,\n",
       "         -0.0027,  0.0036,  0.0116, -0.0117, -0.0002, -0.0108, -0.0022,  0.0084,\n",
       "          0.0142, -0.0173,  0.0015,  0.0081,  0.0078, -0.0091, -0.0049, -0.0130,\n",
       "          0.0021, -0.0062,  0.0174, -0.0124, -0.0022, -0.0031, -0.0092,  0.0033,\n",
       "         -0.0004,  0.0045, -0.0021,  0.0058, -0.0104,  0.0185, -0.0023,  0.0204,\n",
       "          0.0066,  0.0034,  0.0033, -0.0183, -0.0067, -0.0011,  0.0046, -0.0152,\n",
       "          0.0144,  0.0047, -0.0079, -0.0069,  0.0053,  0.0002,  0.0051, -0.0036,\n",
       "         -0.0012,  0.0197, -0.0036, -0.0147, -0.0131,  0.0070,  0.0072,  0.0198,\n",
       "         -0.0028, -0.0009,  0.0171, -0.0053,  0.0027, -0.0115, -0.0021,  0.0066,\n",
       "          0.0006,  0.0081, -0.0077,  0.0159, -0.0171,  0.0037,  0.0133, -0.0128,\n",
       "          0.0039,  0.0128, -0.0064, -0.0056, -0.0286, -0.0005, -0.0089,  0.0022,\n",
       "          0.0055,  0.0051,  0.0136, -0.0020, -0.0066, -0.0235,  0.0017,  0.0102,\n",
       "          0.0117, -0.0262, -0.0057,  0.0078, -0.0004,  0.0004, -0.0002, -0.0225,\n",
       "          0.0140,  0.0109, -0.0112,  0.0122, -0.0072,  0.0190, -0.0080,  0.0034]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature[1].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb595221-3eb6-47da-9b38-a064c0c870e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5038e-02,  1.8302e-02, -2.1734e-02,  7.2005e-04, -4.8347e-03,\n",
       "         -4.2571e-03,  1.5044e-02, -6.2110e-03, -2.9352e-02,  7.1751e-03,\n",
       "         -1.6999e-02,  8.2798e-03, -2.4364e-02, -7.2986e-03, -9.6960e-04,\n",
       "          1.2887e-02, -4.9133e-03,  6.8969e-03, -2.5354e-02, -4.7589e-03,\n",
       "          2.2000e-03,  1.2582e-02, -7.5322e-03, -4.1248e-03, -4.1146e-03,\n",
       "          3.7817e-03,  1.2452e-02, -1.3384e-02,  2.9072e-04, -1.2493e-02,\n",
       "         -3.0532e-03,  1.0106e-02,  1.6605e-02, -1.9927e-02,  2.1151e-03,\n",
       "          8.8946e-03,  8.0261e-03, -1.0808e-02, -5.3365e-03, -1.4784e-02,\n",
       "          2.6951e-03, -7.0208e-03,  1.8336e-02, -1.4516e-02, -2.9461e-03,\n",
       "         -4.4199e-03, -1.0087e-02,  3.0775e-03,  2.3446e-04,  4.9566e-03,\n",
       "         -2.9479e-03,  6.8291e-03, -1.1866e-02,  2.0562e-02, -3.2424e-03,\n",
       "          2.1973e-02,  7.3106e-03,  4.4568e-03,  4.3559e-03, -1.9606e-02,\n",
       "         -6.9541e-03, -7.2710e-04,  5.0061e-03, -1.6544e-02,  1.6785e-02,\n",
       "          6.1018e-03, -7.7099e-03, -8.5416e-03,  5.8827e-03,  1.4781e-03,\n",
       "          5.5733e-03, -3.8125e-03, -9.3343e-04,  2.1874e-02, -3.7143e-03,\n",
       "         -1.6738e-02, -1.4876e-02,  8.7733e-03,  8.1357e-03,  2.2453e-02,\n",
       "         -3.2848e-03, -5.9679e-04,  1.8952e-02, -5.1880e-03,  2.9022e-03,\n",
       "         -1.2379e-02, -1.5050e-03,  6.8921e-03,  5.4173e-04,  8.9379e-03,\n",
       "         -8.1089e-03,  1.6857e-02, -2.0491e-02,  5.1367e-03,  1.5326e-02,\n",
       "         -1.3102e-02,  4.0239e-03,  1.3557e-02, -8.1754e-03, -5.3019e-03,\n",
       "         -3.1200e-02, -2.0270e-05, -1.0254e-02,  2.0934e-03,  6.0781e-03,\n",
       "          4.9396e-03,  1.4964e-02, -2.4499e-03, -6.5694e-03, -2.6086e-02,\n",
       "          2.3498e-03,  1.1234e-02,  1.2600e-02, -2.8615e-02, -6.0700e-03,\n",
       "          9.5322e-03,  4.8488e-04,  6.3824e-04, -1.3949e-04, -2.4836e-02,\n",
       "          1.6105e-02,  1.1717e-02, -1.2416e-02,  1.4048e-02, -8.4899e-03,\n",
       "          2.1804e-02, -8.2278e-03,  4.0844e-03]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature[0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bb3e9c-83ce-4b73-85b7-d61bee49f7b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
